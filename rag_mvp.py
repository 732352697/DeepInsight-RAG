import os
import sys

# --- 1. ç¡®è®¤èº«ä»½ (ç”¨çš„æ˜¯ Python 3.10 å—ï¼Ÿ) ---
print(f"ğŸ å½“å‰è¿è¡Œç¯å¢ƒ: {sys.version.split()[0]}")

# --- 2. å¼•å…¥ RAG çš„æ ¸å¿ƒç»„ä»¶ ---
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate

# å…¼å®¹æ€§å¤„ç†ï¼šé˜²æ­¢ä¸åŒç‰ˆæœ¬æ‰¾ä¸åˆ°åˆ‡åˆ†å™¨
try:
    from langchain_text_splitters import CharacterTextSplitter
except ImportError:
    from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# --- 3. é…ç½®å¯†é’¥ (è¯·å¡«å…¥ä½ çš„ Key) ---
os.environ["OPENAI_API_KEY"] = "f6f8ae4056dc40cd942e8b610af57a62.I3nn1I5xBZLxuLlU"
os.environ["OPENAI_BASE_URL"] = "https://open.bigmodel.cn/api/paas/v4/"

print("\nğŸš€ RAG ç³»ç»Ÿæ­£åœ¨å¯åŠ¨...")

# --- 4. å‡†å¤‡â€œç§æœ‰æ•°æ®â€ ---
# è¿™æ˜¯å¤§æ¨¡å‹åŸæœ¬ç»å¯¹ä¸çŸ¥é“çš„ä¿¡æ¯
text_data = """
ã€æœªæ¥ç§‘æŠ€å–µå–µå¸å‘˜å·¥æ‰‹å†Œ v1.0ã€‘
1. ä½œæ¯æ—¶é—´ï¼šæˆ‘ä»¬å®è¡Œâ€œç¡åˆ°è‡ªç„¶é†’â€åˆ¶åº¦ï¼Œæ—©ä¸Š 11:00 å‰ä¸è®¸åˆ°å…¬å¸ï¼Œä»¥å…æ‰“æ‰°å‰å°æ©˜çŒ«ç¡è§‰ã€‚
2. ç¦åˆ©å¾…é‡ï¼šæ¯ä½å‘˜å·¥å…¥èŒå³é€ 500 ç½é¡¶çº§é‡‘æªé±¼ç½å¤´ï¼Œè™½ç„¶æ˜¯ç»™çŒ«åƒçš„ï¼Œä½†å‘˜å·¥æƒ³åƒä¹Ÿä¸æ‹¦ç€ã€‚
3. æ ¸å¿ƒä»·å€¼è§‚ï¼šåƒçŒ«ä¸€æ ·å¥½å¥‡ï¼Œåƒç‹—ä¸€æ ·å¿ è¯šã€‚
"""

try:
    # --- 5. RAG å››æ­¥èµ° ---

    # æ­¥éª¤ A: åˆ‡åˆ† (Splitting)
    print("1ï¸âƒ£ æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£...")
    docs = [Document(page_content=text_data)]
    text_splitter = CharacterTextSplitter(separator="\n", chunk_size=100, chunk_overlap=0)
    split_docs = text_splitter.split_documents(docs)

    # æ­¥éª¤ B: å‘é‡åŒ–ä¸å…¥åº“ (Indexing)
    print("2ï¸âƒ£ æ­£åœ¨å»ºç«‹å‘é‡æ•°æ®åº“ (Chroma)...")
    embeddings = OpenAIEmbeddings(model="embedding-2")  # æ™ºè°±çš„å‘é‡æ¨¡å‹
    # å»ºç«‹å†…å­˜æ•°æ®åº“ï¼Œä¸å­˜æ–‡ä»¶ï¼Œé€Ÿåº¦å¿«ä¸”ä¸æŠ¥é”™
    vectorstore = Chroma.from_documents(documents=split_docs, embedding=embeddings)
    print("âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼")

    # æ­¥éª¤ C: æ„å»ºæ£€ç´¢é“¾ (Retrieval Chain)
    print("3ï¸âƒ£ æ­£åœ¨ç»„è£… AI æ€è€ƒé“¾æ¡...")
    llm = ChatOpenAI(model="glm-4", temperature=0)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})  # åªæ‰¾æœ€ç›¸å…³çš„ä¸€æ¡

    # å‘Šè¯‰ AIï¼šå¿…é¡»æ ¹æ®ä¸Šä¸‹æ–‡å›ç­”
    prompt = ChatPromptTemplate.from_template("""
    ä½ æ˜¯ä¸€ä¸ªä¼ä¸šåŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„ã€ä¸Šä¸‹æ–‡ã€‘æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    å¦‚æœä½ ä¸çŸ¥é“ï¼Œå°±è¯´ä¸çŸ¥é“ã€‚

    ã€ä¸Šä¸‹æ–‡ã€‘ï¼š
    {context}

    ç”¨æˆ·é—®é¢˜ï¼š{input}
    """)

    combine_docs_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, combine_docs_chain)

    # --- 6. æœ€ç»ˆæé—® ---
    query = "å…¬å¸çš„ç¦åˆ©å¾…é‡æœ‰ä»€ä¹ˆï¼Ÿ"
    print(f"\nğŸ™‹â€â™‚ï¸ ç”¨æˆ·æé—®ï¼š{query}")

    response = rag_chain.invoke({"input": query})

    print("\nğŸ¤– AI å›ç­”ï¼š")
    print("=" * 30)
    print(response["answer"])
    print("=" * 30)

except Exception as e:
    print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")